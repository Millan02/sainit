AWS Is a cloud service provider : 


Cloud Computing : NIST (National instiyute of standard and technology)..  : 

https://www.nist.gov/system/files/documents/itl/cloud/cloud-def-v15.pdf

Servers, Applications, Network, Database, storage..!!
--> AWS, Azure, GCP, orclae cloud, Rackspace, virtusteam, fujitsu, alibaba, tencent cloud, cloud9, Dell bhoomi..

--> On-demand : 
--> Broadnetwork access and High availability : 
--> Resource pooling : 
--> Rapid elasticity : (Scale up and Scale down)
--> Measured Services : 

Tenancy : 
Dedicated tenancy
Shared tenancy

Host Based Hypervisor :   H/w --> OS --> Virtualisation s/w --> o/s , o/s
Bare Metal Hypervisor :  H/W --> Hypervisor --> O/s, o/s



Deployment Models of the CLoud COmputing : 

Public CLoud : Open for everyone.. : AWS, Azure, GCP..  Maintenace : AWS.. 

Private Cloud : Dedicated for one Organisation.. : Maintenace: Your org / 3rd party..!!

Hybdrid Cloud : COmbination of multipe CLoud platforms : 
aws + azure : yes
aws + Onpremise : yes

COmmunity Cloud : Group of org build the infra, those group of org use that infra.. 

__

Service Model of the CLoud COmputing : 

SaaS : Lightsail
PaaS : Beanstalk
IaaS : ec2



How to Create an AWS Account.!!

https://aws.amazon.com/free

Step 1 : Provide Email, Password, COnfirm apssword, Account Name.

Step 2 : COntact Information
Name, Address, Ph number.

Step 3 : Payment information : CC/DC : Visa, master, AMEX, Discovery
2 INR.. Refund back..

Step 4 : Identity Verification : Phone verification and email verification.
Phone : Enter ph no , Call me now --> XXXX --> ANswet the call and enter XXX pin incall..

Step 5 : CHoose Support Plan  : BASIC

1. Account and Biling related : 24x7 support for all SP
2. Service limit increase : 24x7 support for all SP
3. Technical Support : Depends on SP

Basic SP : Free, No technical Support.. 7 core area checks on Trusted advisor
AWS Developer Forums, Knowledgebase articles (KBs)

Developer SP : 29$/Month.. 12-24 Local business hours AWS Support Associate.. 
7 core area checks on Trusted advisor.. Unlimited tickets / Primary contact can raise ticket..
Severity :
General guidance : < 24 Hrs
System impaired : < 12 Hrs

Business SP : 100$/Month.. 24x7 Phone, email, chat support from AWS Engineer.. 
Full trusted advisor checks.. Unlimited tickets/ Unlimited user can raise ticket..
Severity : 
General guidance : < 24 Hrs
System impaired : < 12 Hrs
Production system impaired : < 4 hrs
Production system down : < 1 Hr

Enterprise SP : 15k/Month.. 24x7 Phone, email, chat support from Sr. AWS Engineer.. 
Full trusted advisor checks.. Unlimited tickets/ Unlimited user can raise ticket..
--> Operational and Architectural reviews
--> Dedicated TAM (technical account Manager)
Severity : 
General guidance : < 24 Hrs
System impaired : < 12 Hrs
Production system impaired : < 4 hrs
Production system down : < 1 Hr
Business Critical system down : < 15 min

Review. 

Login to aws account.

_______________________________________________________________________________________
D: 22/05/2021

Enabled MFA on root account. 
--> Virtual MFA
--> u2f key / yubykey 
--> Hardware mfa

Root user : Unrestricted access on aws account. (Closing account, account transfer, Support plan change)

IAM : Identity and access management : used to perform day to day activities.


s3 : Simple storage service : AWS version of gdrive. 
ec2 : Elastic compute cloud :  We can launch and run servers : 


Least privilages mechanism : Provide the access to perform his job/duties.

avinash : s3 full access : IAM : Share his cred
anudeep : ec2 full access : IAM : 
 : TL : Admin access : 

IAM User creation : 

Step 1 : Choose User name 	: sai

Step 2 : Choose access type	: 

--> Management console : GUI : un, pwd & sign-in : Browser
--> programatic access : CLI : Accesskeyid & Secretaccesskey : sdk, cdk, api, 3rd party

Step 3 : Allocate permissions

--> Create a group, Add permissions at group level.. Add this user to group.
--> COpy settings from another IAM user.
--> Attach permissions on user.

Policy : Set of permissions on our aws account.

Step 4 : Add Tags

Step 5 : review and create


Birth settings : 
--> MFA
--> Setup strong password policy for IAM users


Policy : 
--> AWS Managed policy : We cannot delete
--> AWS Managed - Job function : we cannot delete
--> Customer Managed policy : 

** DENY always have highest priority. 
If you allow 100 time, but denied 1 time = DENY


Req: Provide access on S3 and ec2 services.

Requirement : IAM user with Administrator access.. but not on s3.. 


Implicit allow : What are all the services access we provide, he can access only those services. Remaining all services will deny automatically.

Explicit Deny : What are the service set as deny, that will be denied.. If you allow at any level (user, group, resurce), Deny will take effect.

Policy simulator : It can simulate IAM users permissions.

Cloudtrail : Logging service in AWS. It stores last 90 days activities. 

Credentials Report : Report on When user last logged in, When user created, password last used, password reset date.

arn : Amazon resource name : arn name gives uniqueness to our aws resources.


Task 1 : Create an IAM user with S3 fullAccess and login as IAM user and verify his access on S3, ec2 and iam. 	Exp: He should able to see only s3 w/o error.

___
Task 2 : Create an IAM user, with "AdministratorAccess". (use the same user througout course).

Task 3 : Login as task2 user.. And try to access "billing Information", It will show error/access denied.
Provide "billingaccess" to IAM user.
___

Task 4 : Create a new IAM User with S3 FullAccess.. 
Create a Policy to activate MFA himself.

___________________________________________________________________________________
Permissions Boundary : We can setup maximum permissions for an IAM user. 


Inline Policy : Policy for one single resource only. We cannot reuse this policy.


_______________________________________________________________________________________

S3: Simple Storage Sevice : 

--> Object based storage : We can store flat files / any files.
--> We store data in "Buckets"
--> Bucket nothing but a folder with unique name.
--> We can upload objects into bucket.
--> We can create 100 buckets in s3 platform by defaultly. (TO increase count raise service limit increase ticket)
--> Bucket naming Limitation :
	--> Should not start with . / Should not end with . / no adjesent ..
	--> Only Small characters / No Capital letters
	--> bucket name should not resemble IP address format.

--> S3 have unlimited data.
--> S3 supports max Object size : 5 TB, Min Obj size: 0/1 byte

Publicly accessable S3 Object URL : 
https://s3.regioncode.amazonaws.com/bucketname/objectname

Virtual Path: Works when we don't have . in the bucket name

https://bucketname.s3.regioncode.amazonaws.com/objectname
https://bucketname.s3.amazonaws.com/objectname


To make any object public, We need to Modify the bucket level settings, then object level settings.

Block all public access : --> OFF
Go to object promperty --> Make Public. 

___

S3 Standard : Default storage class.. Designed to store Frequently accessed data.. 
Data will be available immediately.. 
Data Spread across >= 3 AZs..
Availa: 99.99% , DUrability : 99.999999999% (11 9's)

Standard IA : Designed to store less freq accessed data.. 
Data will be available immediately.. 
Data Spread across >= 3 AZs..
Availa: 99.9% , Durability : 99.999999999% (11 9's)

OneZone-IA : Designed to store less freq accessed data.. 
Data will be available immediately.. 
Data Spread across 1 AZs..
Availa: 99.5% , Durability : 99.999999999% (11 9's)

GLacier : Designed to store data for longer durations.. FOr Archiving the data..
Data will not be avaibale immedetaly. We need to initiate "Restoration". 
Restoration takes MINUTES - HOURS.
Data Spread across >= 3 AZs..
Availa: 99.99% , Durability : 99.999999999% (11 9's)

Glacier Deep archieve : Designed to store data for longer durations.. FOr Archiving the data.. Data will not be avaibale immedetaly. We need to initiate "Restoration". 
Restoration takes Min HOURS.
Data Spread across >= 3 AZs..
Availa: 99.99% , Durability : 99.999999999% (11 9's)

Interlligent Tier : When we have unknown access patterns for our data.. No idea on how freq we access data.. 
Data Spread across >= 3 AZs..
Availa: 99.9% , Durability : 99.999999999% (11 9's)

RRS : Redused Redunancy storage : Not used : Chances to loss the data is more.. it cost more than S3 standard.. Easily reprodusable data we can store here.. Logs/thumbnails.. 
Durability is Justn 99.99% only.. 

_____

Retrieval tier

Bulk retrieval : Typically within 5-12 hours.
Standard retrieval: Typically within 3-5 hours.
Expedited retrieval : Typically within 1-5 minutes when retrieving less than 250 MB.

_____

FREE TIER ELIGIBILITY : 5 gb Standard storage.. 
2000 PUT Operations, 20,000 GET Operations comes under free tier.. / Month

_______________________

Amazon Segrigated entire world into regions. They named all the regions.

Region : geoGraphical location / Physical location.  25 Regions (May 2021)
Mumbai : ap-south-1
Hyd : ap-south-2
N V : us-east-1

Availability ZOne / AZs : Data Center / Combination of Multiple DCs.. Our resources actually runs here (instance, db).. 80 AZs..
--> Every Region contains Min of 2 AZs. 

Mumbai : ap-south-1a, ap-south-1b & ap-south-1c
hyd : ap-south-a2, 2b & 2c

Edge locations : CDN Endpoint / Cache locations.. 230+ Edge locations.. 

_______________________________________________________________________________________
D: 24/05/2021

Versioning : 
--> We can track all changes happening on this file.
--> If we accidentally deleted the object, we can get it back if versioning is enabled.

Most recent uploaded object called as "Current Object / Latest object"

(version: hide)
When versioning is enabled, If we delete the object, we will get the "Delete Marker", Delete the Delete marker to get this object back to s3 bucket.

(version: show)
Whatever we deletes, we won;t get any delate marker. Object deletes permanently.


delete : Create a Delete Marker.
permanently delete : THis object deletes permanently, without delete marker.

_____
LCR : LifeCycle Management Rule : 

expire : We will get a Delete marker.


_____

CRR/SRR : 
--> Source bucket and Destination bucket should enabled with Versioning.
--> If source bucket and destination bucket is in same region : SRR (Same region replicaiton)
--> If source and destination buckets are in diff region : CRR (Cross region replication).

RTC : Replication Time Control : Boost up the replication process. Max of data replicates within 15 min. 

--> Existing objects won't replicate to destination bucket. All future/subsequent uploads will replicate to destination bucket.

______
Create a Billing Alarm to get notification when any of the aws resource start costing us.
Billing preference --> Receive Billing Alerts -->  Manage Billing Alerts  --> create a billing alarm.

________________

Intelligent tier :

Archive Access tier : When enabled, Intelligent-Tiering will automatically move objects that haven’t been accessed for a minimum of 90 days to the Archive Access tier.

Deep Archive Access tier : When enabled, Intelligent-Tiering will automatically move objects that haven’t been accessed for a minimum of 180 days to the Deep Archive Access tier.

______________

Server access logging : Enable logging for this bucket. 

______________

Event : When any specific operation happened on our s3 bucket, we can trigger another services.

--> SNS : Simple Notification Service
--> SQS : Simple Queue Service
--> Lambda 

Step 1 : create an SNS topic, Add subnscribers to SNS topic. (Set permissions to everyone).
Step 2 : Configure Events via SNS.

_________________________________________

Task 1 : ENable Versioning, Explore versioning.

Task 2 : Configure LCR to Transit all Current Versioned object "S3-Standard" --> Standard-IA after 10 Days --> Delete after 20 Days.
100 Days --> Delete after 200 Days.

Previous versioned objects : S3-Stanard --> Delete after 1 Day.

Task 3 : Create "bucket 1" and "bucket 2" in Mumbai region.. Configure the Replication between these buckets.

Task 4 : Create an SNS topic, add email as subscribers.. COnfigure "PUT" Event notification on s3 bucket.

_________________________________________

Transfer acceleration : S3 uses nearby edge locations to upload / download the data if this feature is enabled.

When geograhical distance increase, latency also increases.
We have tracert / traceroute command to know the internet route.

https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html
--> We should not have . in the bucket name to use this feature.

___

Object Lock : Security feature aws introdused recently. 
--> We need to enable this while creating s3 bucket.
--> For an existing bucket, if you want to enable, we need to contact AWS.
--> Versioning must be enabled. 

Governance : Users with specific IAM permissions can overwrite or delete protected object versions during the retention period.

Compliance : No users can overwrite or delete protected object versions during the retention period. (inclusing root user).

__

Requester pays : Instead of bucket owner, data requester need pay. Requester will pay for requests and data transfer. While requester pays is enabled, anonymous access to this bucket is disabled.

___

Static website hosting : 
--> S3 supports only static website hosting.
--> if future if you want to map a DOmain Name, 
--> DOmain name should be same as Bucketname.
--> Most of the times it's written in html.
--> S3 bucket data should be made as "Public Read", then only this feature works.

http://avinash.com.s3-website.ap-south-1.amazonaws.com


http status codes : 

2XX : OK / Success
3XX : Redirection error
4XX : CLient Side error
5XX : Server Side error

____

WE have a dedicated service called "Cloudwatch" to provide metrics on our aws resources.
--> It provides graphical overview for the resources.


Storage Class Analysis : Analyze storage access patterns to help you decide when to transition objects to the appropriate storage class.

Inventory configurations : You can create inventory configurations on a bucket to generate a flat file list of your objects and metadata. These scheduled reports can include all objects in the bucket or be limited to a shared prefix.


metadata : data about the data. Using Tags we can provide metadata to any of the resource in AWS.

Task : Create a static website and deliver. 
https://www.free-css.com/free-css-templates

Task 2 : Try Object lock feature.

________________________________________________________________________________________

D: 25/05/2021

Encryption : 

--> In-Transit Encryption
	--> S3 Uses SSL / TLS Certificates. THis enabled by defaultly.

--> Server Side Encryption (SSE) / At-Rest 
	--> SSE - S3 : Key material generated and managed by S3 platform. Whoever have valid access on s3 platform they can decrypt the data.

    --> SSE - KMS (Key Management Service): 
	--> aws/s3 : (Default Master Key / DMK) : Key material generated and managed by KMS Service. Whoever have valid access on s3 platform they can decrypt the data.
	--> Customer managed keys (CMK) : Key material generated and managed by KMS Service. We need to provide required permissions on Encryption key to IAM users/Roles, then only they can decrypt the data. (NO FREE TIER ELIGIBILITY : 1$/Month)

	--> SSE - C (Customer Provided) : We/customer need to generate the key material and need to upload the key material to KMS service, then we can use as regular kms key. We need to provide required permissions on Encryption key to IAM users/Roles, then only they can decrypt the data. (NO FREE TIER ELIGIBILITY : 1$/Month)


Client SIde Encryption : When client don;t want to send any unencrypted data to any of the outside env, We can use this CSE. AWS is not responsible for this CSE. (truecrypt, securedata)

_____
KMS Defaultly generates the Key for the services we are using (Ex: aws/s3, aws/ebs, aws/efs). These keys we cannot delete, It is managed by AWS KMS service.

Step 1 : Symmetric 
Symmetric : A single encryption key that is used for both encrypt and decrypt operations
Asymmetric : A public and private key pair that can be used for encrypt/decrypt or sign/verify operat.

Step 2: provid name and descryption.

Step 3: Define key administrative permissions. 
	--> Who can administrate this encryption key. (Delete, Provide permission to users, key disable/enable).

Step 4 : Define key usage permissions
	--> Who can use this key. Who can decrypt the data. 

Step 5 : Review and create.

___________

Bucket Policy : 

Avinash_T : S3 Full Access : 
testuser : S3 Full Access :

1avinash : testuser should not get any chance to upload the data. / Deny "Upload/PUT" for testuser to 1avinash bucket..!!!

--> Bucket Policy generator: https://awspolicygen.s3.amazonaws.com/policygen.html

effect : ALLOW / DENY	: Deny
Principal : For what IAM user ARN : arn:aws:iam::518084852393:user/testuser
Actions : put Object
Resource ARN : On what s3 bucket : arn:aws:s3:::1avinash/*


/*	--> If it is an Object level operation.
	--> If it ia a bucket level operation.

_________

AWS Import / Expore : When we have large set of data to upload/download from AWS, we can use AWS own devices. AWS Snowfamily devices.

AWS Snowcone : 8 TB
AWS Snowball edge : 40 TB - 100 TB
AWS Snowmobile : PB Scale container


digiglobe => Snowmobile 
1 PB = 1 gbps --> 20-22 Yrs to uplaoad the data
Within 3 Months We can migrate 1 PB data

_____________________

User1 : S3 FUll ACCESS
User2 : S3 Full Access

"Sai-Bucket"

FOr USer1 --> DENY "Delete Bucket", "Deny PUTObject", "Deny view/putBUCKETPolicy"
For User2 --> He should able to upload the data, Should able to view bucket policy.

____

You have multiple buckets in S3 Platform.. You want to Allocate "user1bucket" to "user1" only... 
https://aws.amazon.com/premiumsupport/knowledge-center/s3-console-access-certain-bucket/


____________________________________________________________________________________________________________________________________________________________________

D: 26/05/2021


S3 Performance : 
--> If we are storing all the data in an s3 bucket.. 3500 PUT/Sec... 5500 GET/Sec.. 

These value applicable per prefix (Folder):

Per second 35,000, Create 10 prefixes and upload the data..

Bucket/FOlder1 -->  3500 PUT/Sec... 5500 GET/Sec..
bucket/Folder2 -->  3500 PUT/Sec... 5500 GET/Sec..

Increase the randomness on the object names for better performance. 
Use folder/Prefixes in s3 to get maximum performance.

_____

Read after writre consistenacy for put of new objects.
Eventual consistency for overwrite of PUTS and Delete. (it will take some seconds to delete the backend copies).

______________________________________________________________________________________________________________________________________________________________________


EC2 : Elastic Compute Cloud : Where we launch our instances.

Instance = Server (It contains OS, CPU, RAM/Memory, Storage, Network)

Traditional Method : We need to purchase the serverfirst.. Server Rack.. Power connectivity, Cooling systems, Physical security, Network devices.. 
--> Install Operating system, APplications/webservers.. Deploy your code --> Application deliver to end customer.
--> Once you done with the requirement, what you do with the resources..????

--> Launch an ec2 instance --> COnnect to Instance --> Deploy your code --> Deliver your application.
--> Once you done with the requirement, Terminate the respource and pay for the duration you have used. (pay-as-you-go).


On-Demand ec2 instances : When we don't have stable workloads, If you want to test your application for the first time, Non predictable workloads.. 
Price: /Hr basis.. (/Sec basis also)

Reserved ec2 instances : If we have predictable workloads.. If you want to run application for longer durations with aws.. COst less compared to On-demand.
Duration: 1 yr / 3 yrs.

	Standard RI : We cannot change the configuration.
	COnvertable RI : We can chnage the configuration during the period.
	Schedured RI : When we have persistant requests / Repeated requests with in duration (Ex: Weekly 40 hrs.. MON-FRI, Monthly 1-15th)

Pricing : Full Upfront : Need to pay 100% amount as onetime. No need to pay again.
	  Partial upfront : Pay 30-50% as onetime cost, Pay remaining amount on monthly basis based on your usage.
	  No upfront : No need to pay anything as upfront. Pay everything monthly basis.


Spot ec2 instances : If we have flexible start and stop durations, then only we choose spot instances. IF price is increased by AWS, we lost our ec2 instance. We don't run production worklods / Imp data on this spot instances.

1$/hr.. Hey aws, your cost is too high,, if you are okay to give this instance at 0.50$ from today to 30/05/2021. i will take this instance.. : BID to aws.
--> 
if your bid price is more than aws spot price or equal to aws spot price, we will get ec2 instance.

--> If Price is increased and AWS terminated our instance, we need to pay for only completed hours, not for the partial hours.
--> If Price is Not increased and YOU terminated our instance, we need to pay for total duration.

1 Hr 30 Min : YOu teminated : 1 Hr 30 Min
1 hr 30 Min : AWS terminated : 1 Hr

FREE TIER : 750 Hrs/Month with t2.micro WIndows Instance
750 Hrs/Month with t2.micro Linux Instance

__________________________________________________________


Step 1 : Choose an AMI (Amazon machine Image)		: Operating System

Step 2 : Choose Instance type (vCPU, Memory)		: COnfiguration

Step 3 : COnfigure additional settings

--> No of req instances, IAM Role, VPC, Userdata 
--> Shutdown behavior : What should happen when we choose shutdown option at OS level : STOP / Terminate.  : STOP.
--> Enable termination protection : ENABLE

Step 4 : Choose the STorage				: Storage for instance

Step 5 : Add required Tags (Key and Value)

Step 6 : Choose the Security Group 			: Firewall at instance level.

Windows : RDP (Remote Desktop Protocol) : 3389 : 
Linux : SSH (Secure Shell) : 22
http : Hypertext transfer protocol : 80
https : Secure over http : 443

Custom : Any network IP address
Anywhere : 0.0.0.0/0
my IP : Currently connected network public IP address. 

CLient Network IP : 202.153.35.250/32

Step 7 : Review and launch instance using a Keypair (.pem)

Keypair works with Encryption Decryption mechanism. 
--> AWS Keeps the Public Key and it copy the public keyin our dc2 instance.
--> We will have Private Key. If you want to get the password to get connect to the instance, Browse the "privatekey", then only we will get Password.


--> EC2 is a Region specific service. 
WIndows Instane Launch: 

t2.micro : Only t2.micro comes under free tier eligibility (1 vCPU, 1 RAM)

______


General Purpose : Stable/balanced performance of compute, memory and network resources.
Type : t2, t3, m5

Compute Optimized : We will get more CPU performances from these instances. We will have high perf processors in these instances.
Type : c4, c5, c6  (Compute / CPU)

Memory Optimized : We will get more RAM perf. Workloads required to process large set of data via memory.
Type : r4, r5, r6, x1, z1 (RAM)

GPU Optimized / Accelerated computing : We will get more graphic processings, Efficient for data pattern matching, High level gaming.
Type : p2, p3, p4, g3, g4, f1

Storage optimized : we will get more Storage/ Hard Disk performance. FOr the application required more IOPS, we use this types.
Type : d2, d3, i3

___________________________

To Connect to Windows Instances : 

Windows : Open "run" --> Type "mstsc" (Microsoft Terminal Service console) --> Enter.

MacBook : 

Public Ip address : unique across the globe. : Always use Public IP to connect.
Private ip address : unique with in the network.

--> When we connect to the instance, We can Setup customPassword, then we no need to use the keypair.


Task : Launch a Windows Instance, COnnect to it, Change the Wallpaper, Setup custom password.. Disconnect from the instance. 
Try to login to ec2 instance using "Keypair" given password.. Working or not..???
After connecting to instance, verify you have all data / wp or not..???

________________________________________________________________________________________

D: 27/05/2021

Elastic Block Storage : EBS volumes
IOPS : Input and output operations per second

root volume : COntains Operating System : gp2, gp3, io1, io2 & standard
Additional VOlume : st1 & sc1

General Purpose SSD : (gp2 / gp3) : Low latency interactive applications, Dev and test environment..!!
Min : 1 GiB, Max: 16 TiB... Max IOPS: 16,000 IOPS
gp2 --> works 1 : 3 ration (1 gb volume = 3 iops), with min of 100..
for gp3, we have an advantage, we can mention/choose required IOPS count.
__

Provisioned iops : (io1 & io2) : workload that requires Specific IOPS count.. or if we need more than 16,000 iops for our ec2 instance.. I/O Intensive database workloads..
Min : 4 GiB, Max: 16 TiB... Max IOPS: 64,000 IOPS
--> It provided highest performance among all. 
__

Magnetic : Standard : Less freq accessed data, Low cost storage solutions.. 
Min : 1 GiB, Max: 1 TiB...
__

Throughput optimized HDD : st1 : Bigdata, Data warehousing, log processing.. 
Size : Min 125 GiB - 16 TiB.. IOPS : 500.. Throughput : 500 MB/S..

Cold HDD : sc1 : THroughput orientes storages, but with Less Frequently accessed.. Lower cost than st1.. 
Size : Min 125 GiB - 16 TiB.. IOPS : 250.. Throughput : 250 MB/S..

_____________________

Free Tier : 30 Gib /Month.. General purpose and Magnetic Storage.. 

__

--> EC2 instance and EBS volumes should be in same region, THen only we can associate it to ec2 instance. 

We need to make newly added volume available at OS level.

run --> diskmgmt.msc --> Make "online", "Initialise disk", Write File System.

Windows : FAT, FAT 32, NTFS, ReFS
Linux : ext3, ext4, xfs
__

Increase the size of an Existing volume..??
--> First increase size in AWS Console, Then Increase at OS level.

__

VOlume 1 is with Instance 1.. Same volume required for Instance 2..!!! 
--> Yes, It's possible.. But Both the Instances should be in Same AZ.

__

1a-instance and 1b-instance.. 1a instance volume need to detach and it should attach to 1b-instance.. (Our instances are running in different AZs)..!!

Choose the Volume --> Create a Backup (snapshot) --> Create a volume in desired AZ.

Snapshot : Backup copy of our EBS volume.

__

Instance-1 is in Mumbai Region.. Instance-2 is in Another Region..!!!

Choose the Volume --> Create a Backup (snapshot) --> COpy the snapshot to desired region --> Create a volume from copied snapshot in desired Regions AZ.

__

Instance-1 is in Account-1... Instance-2 is in Account-2...!!!! 

Choose the Volume --> Create a Backup (snapshot) --> Choose "Modify permissions" option, Provide the 12 digit aws account id-2 then Save --> Find the snapshot in account-2 region --> Create a volume in desired Regions AZ.

____

You have a volume, that volume you want to share with everyone who have an aws account..!!

Choose the Volume --> Create a Backup (snapshot) --> Choose "Modify permissions" option, select "Public" --> Save

________________

Can we decrease the size of an EBS volume..??
Ans : NO.. Only increase is possible.

_________________________________________________________________________________

Task : Launch an ec2 instance, Make it as webserver, test the output using Public IP address.

Task 2 : Launch an ec2 instance in ap-south-1a.. Create a "New volume with 1 gb size" in ap-south-1a and attach this volume to ec2 instance. Goto AWS console "Increase the volume size to 2 GB", make this 2gb volume available at OS level.

Task 3 : Launch an ec2 instance in ap-south-1a... Another ec2 instance in ap-south-1b..
Create a volume in ap-south-1a.. Attach it to "1a instance" and make it available.. Write some data into it..
Make Same volume available to "1b instance"..!!

______________


How to Make our ec2 instance as Webserver : 

In windows we have "IIS", Install the IIS, your server will become as webserver.

Start --> Server Manager --> "add roles and features" --> "server Role", Select "Webserver (iis)" , "add features" --> next, next, next --> install.

Place all your webdata/webpages : C:\inetpub\wwwroot

Security Groups..  Open port 80 : http.. 

--> Changes to security group takes effect immediately.
_________________________________________________________________________________________

D: 28/05/2021

snapshots : 
--> AWS uses s3 platform to store the snapshots. (We cannot find these snapshots in our s3 buckets).
--> Snapshots are point-in-time copies.

1. DO you want to configure backup for Entire instance (Including all volumes).
2. DO you want to take backup for individual ebs volumes.

DLM : Data Lifecycle Manager : To automate the Snapshot creation for our ebs volumes. 
--> We can take backup of individual ebs volumes / Instances.
--> We cannot share the default encryption snapshots with any aws acocunts/ we cannot make it public.
--> If we have a snapshot with CMK (Customer managed keys), We can share it with Another AWS account user.. But you need to share the Customer managed ebcryption keys. We cannot make it public.

For NonProd : Retention period : 14 Days
For Prod : Retention period : 1 Month

__

Golden AMI : Customised AMI (Amazon Machine Image) : OS

Requirement : 10 ec2 instance.. CUstom Wallpaper.. IIS/webserver.. Putty.. Internal application.. Local users.. With Custom Password..!!!

CIS : Centre for internet security : Standard for every OS.!! : 800-1000 pages doc for every os.

Plain ec2 instance --> Perform all customisation --> Golden AMI (template)

GAMI/Template --> how many instance we launch, we will get all the customized settings.

_______________________________________________________________

Requirement : 10 ec2 instance.. CUstom Wallpaper.. IIS/webserver.. Putty.. Internal application.. Local users.. With Custom Password..!!!

Recommended : Stop ec2 instance, before taking Golden AMI. (not mandatory)

--> GoldenAMIs will have SNapshots in backend. We cannot delete these snapshots.


You have an ec2 instance in Mumbai region, Same need to copied to another region..!! 
--> Create a GAMI of Mumbai region instance, Copy the AMI to desired region.

You have an ec2 instance in Account 1, Same need to copied to Account 2..!! 
--> Create a GAMI of Account 1 instance, Copy the AMI to desired Account 2. (From what region (acc 1) you are sharing, in same region it will copy in acc 2)

We can make these GAMI public.

____

Task : Create a DLM policy to take a backup copy of an ec2 instance, which contains tags as mentioned. and backup retention should be 1 Day. Take backup for everyone hr.

Key 		: Value
Backup_job 	: Application X

Task 2 : create a GAMI, CUstom Password, Custom Wallpaper, Install IIS, Copy some data, Have Additional Volume..

https://www.udemy.com/course/linuxwithavinash/?couponCode=MAY1AVINASH

https://www.udemy.com/course/gitbyavinash/?referralCode=F7D62CDB4D61EADE1CC7

https://www.udemy.com/course/jenkinswithavinash/?couponCode=MAY1AVINASH

_______________________________________________________________________________________

D: 29/05/2021

How to launch and Connect to Linux ec2 instance.

Opensource : We can see how prog working, Modify the prog working structure, We can share it, We can use for any purpose. --> Amazon Linux 2  (Cent OS / Redhat)

Step1 : Choose AMI : Amazon Linux 2 AMI
Step2 : CHoose t2.micro
Step 3 : Additional Configurations 
Step 4 : Choose volume (8 gb volume)
Step 5 : Add Tags
Step 6 : Configure the Security Groups

	Linux : SSH (Secure Shell) : Port 22 : (Anywhere/custom/myip)

Step 7 : create a newkeypair and launch instance. (.pem)

How to connect to Linux Instance.

Windows OS Users : 
Option 1 : Install "Openssh" in windows laptop, use "Command Prompt". (.pem)
Option 2 : DOwnload and run "Putty" application. Putty WOn;t support .pem format file, So 	convert the .pem to .ppk ("PuttyGen")

Mac / Linux OS : We can use Default "terminal". No additional s/w required.

--> Permisisons Error / Permissions are too open : chmod 400 Keypair.pem

_______________________________________________________________________________________

Linux Instances : Default username : ec2-user  / redhat / ubuntu

whoami		--> tels us as what user we are working.
sudo su		--> Switch to root user
exit		--> Exit from root user to ec2-user

ls		--> List the files/folders
ls -a		--> List all including hidden files
pwd		--> Print working directory
mkdir		--> Create a Directory/Folder
touch 		--> o bytes file/ plain file
cd		--> Change directory

copy and paste :  cp
cut and paste  :  mv

VIM Editor : 

vim filename	--> Open this file in VIM editor

Press I		--> INSERT Mode
Press ESC	--> ReadOnly Mode
:wq		--> Write and Quit (Write changes to file and quit the editor)
:q!		--> Quit the Editor without writing the changes
__

Req: Make this linux Instance as Web Server (Apache).

rpm : Redhat package manager
yum : Yellowdog Update manager

yum install httpd -y
service httpd status			--> httpd webserver service status
service httpd start/stop/restart
chkconfig httpd on			--> Makes httpd as logon service.

path : /var/www/html/

cd /var/www/html/

Tool to Move the files to amazon linux ec2 instance : WinSCP
________________________________________________________________________________________


Task 1 : Launch Amazon Linux 2 Instance, Connect to the instance. 
Task 2 : Task : Launch a linux ec2 instance and make it as webserver, deliver the custom webpage.

<html>
<title> Site </title>
<h1> Any text </h1>
</html>

____________________________________________

D: 31/05/2021

lsblk		--> List block based devices

df -Th		--> List the available volumes

/dev/xvda1	--> root volume (/)
/dev/xvdf	--> new Volume name 

In windows OS, we mount new volumes to drive letters.. But in Linux OS, We mount volumes to Directory. 

mkdir newvolume

file -s /dev/xvdf  	--> If this command returns with "data", no file system.
			--> If this returns ext3/xfs filesystem.. We have a file system.


mkfs -t xfs /dev/xvdf		--> Make file system and write it to Additonal volume

mount /dev/xvdf newvolume/	--> mount addl volume to directory

--> Above mount is temp mount only.. it won't available after the instance reboot.
--> To make this volume perm mount to the directory, Add entry in "/etc/fstab" file.
--> Get the entry information from "/etc/mtab" file. 

--> cat /etc/mtab		--> Grab the entry related to volume

/dev/xvdf /home/ec2-user/newvolume xfs rw,relatime,attr2,inode64,noquota 0 0

--> vim /etc/fstab.. Write the above entry, save and quit the editor

__________
--> Increase the volume size the "Cosole" first, then execute the below command..!!

We can use "xfsprogs" to increase existing volume size. 
yum install xfsprogs

--> xfs_growfs -d /volume-Mountpoint
--> xfs_growfs -d /home/ec2-user/newvolume

_______________________

GoldenAMI on Linux : 

How to recover a Linux instance, if we lost Keyapir..!!!
Ans : Choose the Instance --> Create a GAMI --> Launch from GAMI with New Keypair.


Task 1 : Launch linux ec2 instance with 8gb root volume. Add additional "1 gb" volume to ec2 instance and make it available at OS level.

Task 2 : Increase the size of the "1 gb" volume to "2 gb" we used in task 1, make it available.

Task 3 : Install webserver is Linux instance, Create a golden AMI of the linux instance. 
While launching instance from goldenAMi, In final step, choose a new keypair. try to login to instance with both the keypairs. 


________________________________________________________________________________________

D: 01/06/2021

EFS : Elastic File System.. : Centralised storage solutions for ec2 instances. 

--> EFS works on NFS (network file system) v4 protocol.
--> EFS supports only Linux Operating systems. 

_______

Userdata : Bootstrap script : BSS : 

curl http://169.254.169.254/latest/user-data/

Linux : 

#!/bin/bash
yum install httpd -y
service httpd start
chkconfig httpd on
echo "<h1>BSS Webpage</h1>" >> /var/www/html/index.html

Windows : 
<script> commands </script>
<powershell> enter all the commands </powershell>

<script>
tzutil /s "India Standard Time"
</script>

_____________

Metadata on ec2 instances :

Linux :  
curl http://169.254.169.254/latest/meta-data/

Windows : open browser and give url
ami-manifest-path
block-device-mapping/
events/
hibernation/
hostname
identity-credentials/
instance-action
instance-id
instance-life-cycle
instance-type
local-hostname
local-ipv4
mac
metrics/
network/
placement/
profile
public-hostname
public-ipv4
public-keys/
reservation-id
security-groups

___________________

Placement Groups : How ec2 instances placed in AWS physical environment.

--> Cluster PG  : Low network latency and high network throuput
--> Partition PG : HDFS (hadoop distributed file system), Datawaregousing
--> Spread PG

__________________

Elastic beanstalk : PaaS : 

Developers team : php/python : They need platform to delopy the code.. 

_________________

Tenancy : 
--> Shared Tenancy / Default : underlying h/w shares with multiple customers.
--> Dedicated Tenancy : Won't share with anyone. Dedicated to our ec2 instance only.
	--> Dedicated Instance : Less backend visualisation.
	--> Dedicated Host : More backend visualisation, We can apply licenses at hardware level (CPU Cores, Memory slots)


AWS User XEN Hypervisor.. Nitro hypervisor (AWS own)

_________________

Status checks detect problems that may impair/impact our ec2 instance from running your applications.
	--> System Status Check : Underlying hardware level issues.
	--> Instance status check : OS level issues

Stop and Start the instance when system status checks are failing.

________________

Cloudwatch : Monitoring service in AWS environment.
--> Using CW, we can monitor CPU, Network, Disk. 
--> We cannot monitor "Memory (RAM) Usage" of an ec2 instance. (We need to create custom metrics to monitor Memory usage).

2 types of monitorings available in AWS: 
--> Basic Monitoring : Monitoring interval is 5 Min.
--> Detailed Monitoring : Monitoring interval is 1 Min.

_____________________________

ELB, ASG, CLI, Roles, System Manager


























































































































